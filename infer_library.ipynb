{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d48ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Encode text\n",
    "def encode_text(text, bert_tokenizer, bert_model, device):\n",
    "    \"\"\" Text embeddings.\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to convert to an embedding vector.\n",
    "    bert_tokenizer\n",
    "        The BERT tokenizer to encode the `text`.\n",
    "    bert_model\n",
    "        The BERT model that will embed the `text`\n",
    "    Returns\n",
    "    -------\n",
    "    array-like, shape(d_model,)\n",
    "        A vector of d_model dimensions.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            sent_ids = bert_tokenizer.encode(text.lower(), truncation=True))\n",
    "            bert_input = torch.tensor([sent_ids]).to(device)\n",
    "            outputs = bert_model(bert_input)\n",
    "            cls_token = outputs[0][0][0]\n",
    "            emb = cls_token.cpu().numpy()  # Convert to numpy array\n",
    "            # pooler = outputs[1]\n",
    "            # emb = pooler.cpu().numpy()\n",
    "        except Exception as e:\n",
    "            print(len(text))\n",
    "            print(text)\n",
    "            print(e)\n",
    "            pass\n",
    "    return emb\n",
    "\n",
    "# Pad text if it's more or less than 20 sentences\n",
    "def pad_array(array, max_rows, truncate='pre'):\n",
    "    rows, cols = array.shape\n",
    "    if rows > max_rows:\n",
    "        if truncate == 'pre':\n",
    "            return array[-max_rows:, :]\n",
    "        else:\n",
    "            return array[:max_rows, :]\n",
    "    else:\n",
    "        fill_rows = max_rows - rows\n",
    "        if truncate == 'pre':\n",
    "            return np.lib.pad(\n",
    "                array, ((fill_rows, 0), (0, 0)),\n",
    "                'constant',\n",
    "                constant_values=(0))\n",
    "        else:\n",
    "            return np.lib.pad(\n",
    "                array, ((0, fill_rows), (0, 0)),\n",
    "                'constant',\n",
    "                constant_values=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632ea5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Text and Sentence Tokenizer\n",
    "def sent_tokenize(\n",
    "        text,\n",
    "        subst=\"\\n\",\n",
    "        regex=\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
    "):\n",
    "    \"\"\"\n",
    "    A simple sentence tokenizer based on regular expressions to find the\n",
    "    sentence boundaries.\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to tokenize.\n",
    "    subst : str\n",
    "        The substitution character that defines a sentence boundary.\n",
    "    regex : str\n",
    "        The regular expresion to use in order to find sentence boundaries.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list with the sentences of the text.\n",
    "    See Also\n",
    "    --------\n",
    "    `https://stackoverflow.com/a/25736082/1143894`_\n",
    "    Examples\n",
    "    --------\n",
    "    >>> doc = \"MI patients had 18% higher plasma levels of MAp44 \"\\\n",
    "    \"(IQR 11-25%) as compared to the healthy control group (p < 0.001). \"\\\n",
    "    \"However, neither salvage index (Spearman rho -0.1, p = 0.28) \" \\\n",
    "    \"nor final infarct size (Spearman rho 0.02, p = 0.83) correlated \"\\\n",
    "    \"with plasma levels of MAp44.\"\n",
    "    >>> sents = sent_tokenize(doc)\n",
    "    >>> print(len(sents))\n",
    "    2\n",
    "    \"\"\"\n",
    "    substitutions = re.sub(regex, subst, text, 0, re.MULTILINE)\n",
    "    return list(substitutions.split(subst))\n",
    "\n",
    "\n",
    "class SentenceSplitter:\n",
    "    \"\"\"\n",
    "    Splitting sentences using some heuristics that help to prevent wrong\n",
    "    segmentation especially in scientific papers.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> doc = \"MI patients had 18% higher plasma levels of MAp44 \"\\\n",
    "    \"(IQR 11-25%) as compared to the healthy control group (p < 0.001). \"\\\n",
    "    \"However, neither salvage index (Spearman rho -0.1, p = 0.28) \" \\\n",
    "    \"nor final infarct size (Spearman rho 0.02, p = 0.83) correlated \"\\\n",
    "    \"with plasma levels of MAp44.\"\n",
    "    >>> ss = SentenceSplitter()  # Use the module's sent_tokenize function.\n",
    "    >>> print(len(ss.tokenize(doc)))\n",
    "    2\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = sent_tokenize\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def _first_alpha_is_upper(sent):\n",
    "        for c in sent:\n",
    "            if c.isalpha():\n",
    "                if c.isupper():\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _ends_with_special(sent):\n",
    "        sent = sent.lower()\n",
    "        ind = [\n",
    "            item.end() for item in re.finditer(\n",
    "                r'[\\W\\s]sp.|[\\W\\s]nos.|[\\W\\s]figs.|[\\W\\s]sp.[\\W\\s]no.|'\n",
    "                r'[\\W\\s][vols.|[\\W\\s]cv.|[\\W\\s]fig.|[\\W\\s]e..|'\n",
    "                r'[\\W\\s]et[\\W\\s]al.|[\\W\\s]i.e.|[\\W\\s]p.p.m.|[\\W\\s]cf.|'\n",
    "                r'[\\W\\s]n.a.', sent)\n",
    "        ]\n",
    "        if len(ind) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            ind = max(ind)\n",
    "            if len(sent) == ind:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def _split_sentences(self, text):\n",
    "        sents = [l.strip() for l in self.tokenizer(text)]\n",
    "        ret = []\n",
    "        i = 0\n",
    "        while i < len(sents):\n",
    "            sent = sents[i]\n",
    "            while (i + 1) < len(sents) and (\n",
    "                    self._ends_with_special(sent)\n",
    "                    or not self._first_alpha_is_upper(sents[i + 1])):\n",
    "                sent += ' ' + sents[i + 1]\n",
    "                i += 1\n",
    "            ret.append(sent.replace('\\n', ' ').strip())\n",
    "            i += 1\n",
    "        return ret\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\"\n",
    "        Tokenize a text to its sentences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            The text to tokenize.\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of sentences.\n",
    "        \"\"\"\n",
    "        sents = []\n",
    "        subtext = re.sub(r'\\s+', ' ', text.replace('\\n', ' ')).strip()\n",
    "        if len(subtext) > 0:\n",
    "            ss = self._split_sentences(subtext)\n",
    "            sents.extend([s for s in ss if (len(s.strip()) > 0)])\n",
    "        if len(sents[-1]) == 0:\n",
    "            sents = sents[:-1]\n",
    "        return sents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciARK",
   "language": "python",
   "name": "sciark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
